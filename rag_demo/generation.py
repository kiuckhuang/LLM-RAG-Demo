import openai
from .logger import logger, debug_log
from .config import Config

def generate_response(query: str, context: str, model: str = None) -> str:
    """
    Generate response using LLM with provided context
    
    Args:
        query: User query
        context: Retrieved context from documents
        model: LLM model to use (uses config default if None)
        
    Returns:
        Generated response
    """
    if model is None:
        model = Config.CHAT_LLM_MODEL
    
    # Check if we're using placeholder API keys (demo mode)
    is_demo_mode = Config.CHAT_LLM_API_KEY.startswith("demo_placeholder")
    
    logger.info(f"Generating response with {Config.CHAT_LLM_PROVIDER} model: {model}")
    debug_log(logger, f"Query: {query}")
    debug_log(logger, f"Context length: {len(context)} characters")
    debug_log(logger, f"Demo mode: {is_demo_mode}")
    
    if is_demo_mode:
        # In demo mode, provide a simulated response that references the context
        context_preview = context[:200] + "..." if len(context) > 200 else context
        response_text = f"This is a simulated response to your query: '{query}'. In a real implementation, this would be generated by an LLM using the provided context. Here's a preview of the context I would use: '{context_preview}'"
    else:
        # Configure OpenAI client for real API usage
        client = openai.OpenAI(
            api_key=Config.CHAT_LLM_API_KEY,
            base_url=Config.CHAT_LLM_API_BASE
        )
        
        # Create prompt with context
        prompt = f"""
        Use the following context to answer the question according to the best of your ability.
        If the context doesn't contain the information needed to answer the question,
        please say so and only answer based on the given context.
        
        Context:
        {context}
        
        Question:
        {query}
        
        Answer:
        """
        
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=32768
            )
            response_text = response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"Error generating response: {str(e)}")
            response_text = "Sorry, I encountered an error while generating a response."
    
    debug_log(logger, f"Generated response length: {len(response_text)} characters")
    logger.info("Response generation complete")
    
    return response_text

def post_process_response(response: str) -> str:
    """
    Post-process generated response
    
    Args:
        response: Raw LLM response
        
    Returns:
        Processed response
    """
    logger.info("Post-processing response")
    debug_log(logger, f"Raw response length: {len(response)} characters")
    
    # Strip whitespace
    processed_response = response.strip()
    
    # Ensure response ends with proper punctuation
    if processed_response and processed_response[-1] not in '.!?':
        processed_response += '.'
    
    debug_log(logger, f"Processed response length: {len(processed_response)} characters")
    logger.info("Response post-processing complete")
    
    return processed_response
